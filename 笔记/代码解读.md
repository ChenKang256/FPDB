# fpdb-main

## src

### CAFInit.cpp

​	对c++ Actor Framework中所有的actor进行初始化。

### Client.cpp(这个可能比较重要，关系着execute)

​	定义了客户端client的方法，包括获取元数据的路径、启动start、停止stop、重启restart、执行查询executeQuery等方法。

​	executeQuery的流程

+ getCatalogueEntry：获取存放csv等数据的目录入口(与元数据有关？)，用于下一步执行，通过fpdb-catalogue来进行
+ plan
  + 调用calciteClient_->planQuery，利用fpdb-calcite生成执行计划。
  + 调用planDeserializer->deserialize，将执行计划字节流反序列化为执行计划对象
  + prePhysicalPlan->populateAndTrimProjectColumns删除不需要使用的领域
  + prePToPTransformer->transform转变为物理执行计划
+ execute：调用fpdb-executor的方法，执行计划并返回结果

### ClientMain.cpp(这个可能比较重要，关系着execute)

​	主要用于初始化、配置client并不断执行query(通过调用execute方法)

​	execute方法中，会先对第一关键字进行匹配

+ file：读取query file文件执行，executeQueryFile
+ sql：直接读取指令执行，**executeQuery**
+ set：设置配置文件
+ local_ip：不知道是干啥

### ExecConfig.cpp

​	进行全局配置。

### Server.cpp

​	定义了服务器server的方法，分别为server的启动start与结束stop。

### ServerMain.cpp

​	进行配置，并启动server。

# fpdb-catalogue

​	用于获取存放csv等数据与元数据的目录。

# fpdb-calcite

​	用于Apache数据库的Calcite查询引擎生成执行计划。

# fpdb-executor

​	用于根据计划执行操作。

## cache

​	用于将cache层封装为actor。

## message

​	用于各个actor间信息的传递。

## physical

​	各个具体算子的执行。

## executor.cpp

​	定义了执行器executor的若干方法，包括启动start，停止stop，执行execute等。

​	execute通过调用execution的execute方法来得到计划执行的结果。

## execution.cpp

### boot

​	添加算子，并生成actor与collate(？)

### start

​	连接各个actor，对consumer等关系进行初始化，并启动actor开始执行。

### join

​	等待所有actor完成，并返回结果。

# data

​	query在resources/query/tpch/original里面！不是在tpch-dbgen里面。

## customer.tbl

​	`c_custkey, c_name, c_address, c_nationkey, c_phone, c_accbal, c_mktesegment, c_comment`

## nation.tbl

​	`n_nationkey, n_name, n_regionkey, n_comment`

## order.tbl

​	`o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment`

## part.tbl

​	`p_partkey, p_name, p_mfgr, p_brand, p_type, p_size, p_container, p_retailprice, p_comment`

## partsupp.tbl

​	`ps_partkey, ps_suppkey, ps_availqty, ps_supplycost, ps_comment`

## region.tbl

​	`r_regionkey, r_name, r_comment`

## suplier.tbl

​	`s_suppkey, s_name, s_address, s_nationkey, s_phone, s_acctbal, s_comment`

## lineitem.tbl

​	`l_orderkey, l_partkey, l_suppkey, l_linenumber, l_quantity, l_extendedprice, l_discount, l_tax, l_returnflag, l_linestatus, l_shipdata, l_commitdata, l_receipdata, l_shipinstruct, l_shipmode, l_comment`

# 各个actor间message交互流程

​	以testutil为开始来进行分析，并且只挑重要的内容讲述。

## TestUtil.cpp

+ e2eNoStartCalciteServer：运行runTest，来执行testcase
+ runTest：先makeExecutor以生成，再executeQueryFile
+ makeExecutor：在初始化后直接调用start方法，而start不会产生cache的actor，只是将状态running\_设置为true
+ executeQueryFile：用于实际执行，包括初始化路径、生成执行json(可以输出一下，以便知道可能的执行顺序)、反序列化、裁剪、转化(注意此时会加入一个叫做collect的operator，作为根节点。主要调用PrePToPTransformer.transform方法)，再调用executor.execute

## PrePToPTransformer.cpp

+ transform：会对计划中不同种类的operator进行处理，并根据他们间的关系生成consumer与producer关系而形成树状关系

## Executor.cpp

+ execute：先初始化execution，调用execution.execute，再根据需要输出时间或写入等操作

## Execution.cpp

+ execute：依次调用boot、start、join方法
+ boot：初始化时间，告知cache(不需要)，设置query id与添加操作符(在opDirectory\_中添加op名与POpDirectoryEntry的映射)，再在remoteSpawn或localSpawn生成operator。在调用时，会使用PhysicalOp.create方法，可以在此时输出operator的情况
+ start：设置各个算子状态为未完成，再与上游的生产者、下游的消费者进行连接(采用ConnectMessage，封装为Envelope让rootActor\_去发送，使用CAF的anon\_send方法)，再一一启动(利用StartMessage，同样让rootActor\_去发送)。而POpActor的behavior决定了收到信息时会如何执行。
+ join：等待所有算子完成，查看发给root的信息，若完成则结束时间，否则出错

+ localSpawn与remoteSpawn：在local或remote采用actorSystem\_的spawn方法来产生actorHandle，反映的机制基于POpActor的behaviour函数，返回值为收到不同种类与内容信息时对应的反馈

## POpActor.cpp

+ behavior：

  + connect：对于connections的所有成员与本operator建立连接
  + start：设置running\_ = true，调用on\_receive方法，然后不断处理在此期间堆积的message？？
  + stop：设置\_running = false，直接调用on\_receive方法。但似乎没有这玩意？？
  + 根据running\_，分为两种情况
    + 若此时running\_ = false，说明operator被关闭了，message会不断在buffer\_中堆积
    + 若此时running\_ = true，说明operator正常启动。若为complete，则说明发送者已经完成了本operator的计算，此时会将发送者的状态complete设置为true。最后，我们调用on\_receive方法。

  最后，会增加上执行该操作所需时间。

## PhysicalOp.cpp

+ on\_receive的交互流程如下：
  + start：调用on\_start方法，主要是用于输出名字等信息。
  + tuple：调用on\_tuple方法，将上游producer产生的tupleSet塞到自己的buffer里面缓存
  + complete：调用on\_complete方法。当所有上游的producer都完成时，会进行本操作符独有的操作，完成后先向所有下游的consumer发送tuple的message(如有必要)，再发送complete的message

# 并行的流程

## TestUtil.cpp

+ e2eNoStartCalciteServer：调用初始化(会读入并行度，存储在parallelDegree\_里面)，再调用runTest
+ runTest：调用makeExecutor、executeQueryFile
+ makeExecutor：读取remoteIP(注意，此时不会读取到本地的127.0.0.1！)，再调用connect连接远程，生成executor(传入connect生成的用于远程的nodes\_)，再调用start，最后再execute
+ connect：根据remoteIP利用CAF连接远程服务器，将得到的数据结构(vector\<caf::node_id\>)存储在nodes\_中。此时，nodes\_会以index的形式进行访问

+ executeQueryFile：会生成执行计划，根据是否distributed给PrePToPTransformer的numNodes传1(false)或nodes\_.size(true)，再传入parallelDegree\_，调用transform方法(这里是实际生成op，也会决定其是否remote生成、在哪个remote生成进行)

## Executor.cpp

+ 初始化：会将TestUtil.makeExecutor传入的nodes\_赋值给自己的nodes\_
+ start：无关
+ execute：生成Execution(传入nodes\_)，再调用execute进行执行

## Execution.cpp

+ 初始化：读入Executor.execute传入的nodes\_，赋值给自己的nodes\_
+ execute：调用boot、start、join
+ boot：调用remoteSpawn，这是使用nodes\_的地方(每一个op存储的都是nodes\_的index)。只有op的类型在allowedOpTypes(FileScan, Filter, ~~Shuffle, Aggregate~~)、op试图在远程生成(op->spawnOnRemote() == true)、当前模式是分布式(isDistributed\_ == true)，才会调用remoteSpawn
+ start、join：无关
+ remoteSpawn：利用op.nodeId对nodes\_进行索引，得到的数据结构传入CAF的remoteSpawn中，进行远程生成。此后可以利用CAF进行正常通信。

## PrePToPTransformer.cpp

+ nodeId的分配例子见preOp.md

+ 初始化：从TestUtil.executeQueryFile读入预期nodes的数目，传入到numNodes\_中(注意此时不包括本地的127.0.0.1！)，再赋值parallelDegree\_。
+ transformDfs：根据输入的op类型，调用不同的transform方法，返回`pair<vector<shared_ptr<PhysicalOp>>, vector<shared_ptr<PhysicalOp>>>`。推测pair的第一个为某一操作符对应的所有op(因为并行化、remoteSpawn，一个操作符可能生成多个一致的op)，第而个为该操作符上游的所有op+first生成的op。
+ transformProducers：对于输入op的所有producer执行transformDfs操作，将得到的pair压入vector，返回`vector<pair<vector<shared_ptr<PhysicalOp>>, vector<shared_ptr<PhysicalOp>>>>`。每一个pair都对应一个producer，其first就是producer生成的op，second是producer上游所有的op+producer生成的op。
+ 不同transform方法op的nodeId情况：
  + CollatePOp：0
  
  + transformSort：首先，对于预生成的运算符SortPreOp执行transformProducers。由于只会有一个producer，因而我们先判断返回的size是否为1，再取出第一个pair，其first(upConnPOps)为producer生成的所有op，second(allPOps)为包括producer在内的所有上游op。之后正式生成SortPOp并进行连接，最后返回`pair<vector<shared_ptr<PhysicalOp>>{sortPOp}, allPOps+sortPOp>`。
    
    + SortPOp：0
    
  + transformLimitSort：与transformSort类似。
  
    + LimitSortPOp：0
  
  + transformAggregate：多了aggRemoteSpawnInfos，只有idx = 0处这一个元素，当且仅当上游生产者为FILTERABLE_SCAN时为true(代表可以remote生成)。之后，生成parallelAggProjectColumnNames，若上游op个数多于1，则需要根据聚合的functions来处理：
  
    + 若是AVG，则插入SUM+columnName与COUNT+columnName
    + 否则直接插入columnName
  
    否则，parallelAggProjectColumnNames直接设为所需的columnName。之后，针对上游的每一个op，生成与之nodeId相同的AggregatePOp，后面就看不懂了（qwq
  
    + AggregatePOp：upConnPOps[i]->getNodeId()
    + AggregatePOp：0，用于Aggregate Reduce
  
  + transformGroup：与上述类似，区别在于是在每一个node上生成parallelDegree\_个op
  
    + GroupPOp：i % numNodes_
    + ShufflePOp：upConnPOp->getNodeId()
  
  + transformProject：
  
    + ProjectPOp：upConnPOps[i]->getNodeId()
  
  + transformFilter：
  
    + FilterPOp：upConnPOps[i]->getNodeId()
  
  + transformHashJoin：HashJoin需要有两个producer
  
    + shuffleRemoteSpawnInfos：upLeftConnPOp->getNodeId()、upRightConnPOp->getNodeId()，先生成并与上面连接
    + HashJoinBuildPOp：i % numNodes\_
    + HashJoinProbePOp：i % numNodes\_
  
  + transformNestedLoopJoin：与上述类似
  
    + NestedLoopJoinPOp：i % numNodes\_
    + SplitPOp：upRightConnPOp->getNodeId()
  
  + transformFilterableScan：调用了PrePToLocalFSPTransformer的transformFilterableScan方法。这个估计是最重要的，决定了不同的node会读取到哪些文件，从而进行处理再汇总。
    + FileScanPOp(唯一使用处)：partitionId % numNodes\_，按照partition在node上进行分配。同时，能够证明FileScanPOp是叶子节点，也是所有节点启动的源头。
    + FilterPOp：partitionId % numNodes\_，同上



==各个actor的交互逻辑(包括计算节点与存储节点之间，client-server)==

当前是pushdown-only，用于测试operator在smartNIC上的性能

三个模块：test, client, server