# 工作日志

# 2022/07/19

+ 更新filescanpop.cpp中的东西，自测一下，告知师兄！
  + 测一下`‘|’ and ','`：确实不行，会报错。原先的代码没有该问题，因为有这句话：`csvTableFormat->getFieldDelimiter()`
  + 自测不了：缺少`name(), table_->getSchema(), outputSchema`
+ remove all `ifdef __AVX2__`，有报错(S3)可以改CMake或代码，same
  + FileScanPOp.cpp：移除了ifdef
  + FileScanPOp.h：移除了ifdef
  + S3SelectPOp.h：移除了ifdef
  + S3SelectPOp.cpp：移除了ifdef与部分代码
  + S3GetPOp.h：移除了ifdef
  + S3GetPOp.cpp：移除了ifdef与部分代码
  + CSVToArrowSIMDChunkParser.h/.cpp：不需要管，涉及前六个文件的内容已删去
  + CSVToArrowSIMDStreamParser.h/.cpp：同上
  + SIMDParserHelper.h/.cpp：涉及到上面两组CSV文件，不需要管
  + 修改了fpdb-tuple下CMakeLists.txt的第44(CSVToArrowSIMDStreamParser)、46行(CSVToArrowSIMDChunkParser)、45行(SIMDParserHelper)

# 2022/07/20-2022/07/21

+ 统计CSV的算子数目、比例等情况。注意到，当前是pushdown-only的状态，所有的计算只在存储节点上运行。
  + 3与7比较类似，时间与比例都比较接近
  + 7与8形式相似，但是8显著慢于7，注意到8多了一个sum运算，猜测是sum运算开销太大导致速度下降
  + 7、8与9形式相似，但是9显著慢于前二者，这是因为前二者对时间有很强的限制，大大缩小了lineitem的范围，而lineitem作为最大的一个table，对性能影响比较显著，而9一方面hashjoin的范围很大，另一方面sum的求值范围很大，导致速度非常慢
  + 去除1的聚合，从33降低到17；去除8的所有sum，能将运行时间从34降到22；去除9的sum，能将运行时间从45降到26；去除12的sum，能将运行时间从9.8降到9.7。这能够说明sum对运行速度存在影响，并且sum需要的规模越大，对运行时间的影响也越大(百分比)。
  + 结论：普通的运算只要不涉及聚合运算与过于复杂的select、where、order by、group by等，速度都比较快；而涉及到sum、avg这些聚合运算，规模越大对运行时间的影响也越大(百分比)。
  + 感觉sort应该也有很大的影响，但是大家几乎都有sort的前提下，看不出来影响有多大。。
  + 难以看出提升的百分比与算子之间的关系
+ POpActor.cpp，看懂所有operator，client-server的交互(TPCHDIstTest.cpp的debug与test有助于了解)
  + 已在代码解读中完成

# 2022/07/22-2022/07/29

+ SPGLOG写

+ 了解plan的树状结构(输出每个sql的tree)

+ **捋清楚operator是如何在node上分布的，为何能够有性能提高(代码层次)**

  + 

+ scale 1,10,20 tpch size，1/2 node是什么情况，测试每一个sql(是否能够正常完成，为啥无法完成，哪些operator导致无法完成(client？))，纯无remote(注释掉filter与file\_scan)、仅有file\_scan。可以试着改变MainTest的doctest来进行。==优先完成==，用ssb测试
  + ssb生成坑点：
    + O\_CRATE的MODE问题：现在open如果使用了O\_CREAT，必须在后面给权限0777，否则会报错(bm\_utils.c的tbl\_open，retcode)
    + buffer overflow：修改shared.h的第120行，MAXAGG_LEN的定义，改成20。因为给的太小，所以造成overflow
  
+ scale生成：
  
  + stats.json：date不变(2557)，supplier乘上倍数，customer乘上倍数，part的200000加上600000\*log10()，lineorder接近乘上倍数
    + zoneMap.json：让lineorder按照划分数去划分lo\_orderdate，[19920101, 19980802]，2406天
    + schema.json：(这里都是指代)date = 1，supplier = 1，一个customer大概150000，一个part大概150000，一个lineorder大概150000
    + 原有的readme\_new是错误的，每次改变scale factor都必须修改，而不能简单地调用
  
  + 测试时间

    scale = 1，node = 2，有FileScan
  
    | sql名称 | 1.1  | 1.2  | 1.3  | 2.1   | 2.2   | 2.3   | 3.1   | 3.2   | 3.3  | 3.4  | 4.1   | 4.2   | 4.3   | 1    | 2    | 3    | 4    | 5    |
    | ------- | ---- | ---- | :--: | ----- | ----- | ----- | ----- | ----- | ---- | ---- | ----- | ----- | ----- | ---- | ---- | ---- | ---- | ---- |
    | 第一次  | 8.99 | 8.99 | 8.96 | 10.07 | 10.47 | 10.27 | 10.95 | 9.98  | 9.65 | 9.78 | 14.56 | 13.92 | 13.77 | 2.96 | 2.31 | 3.32 | 3.01 | 2.86 |
    | 第二次  | 9.20 | 8.90 | 9.34 | 10.37 | 10.75 | 10.39 | 10.26 | 9.74  | 9.67 | 9.28 | 14.67 | 14.19 | 13.77 | 3.01 | 2.29 | 3.23 | 2.92 | 2.81 |
    | 第三次  | 9.09 | 8.85 | 9.19 | 10.31 | 10.76 | 10.45 | 10.63 | 10.27 | 9.64 | 9.69 | 14.73 | 14.78 | 13.78 | 3.05 | 2.32 | 3.29 | 2.96 | 2.83 |
  | 平均    |      |      |      |       |       |       |       |       |      |      |       |       |       |      |      |      |      |      |
    
    scale = 10，node = 2，有FileScan
    
    | sql名称 | 1.1  | 1.2  | 1.3  | 2.1  | 2.2  | 2.3  | 3.1  | 3.2  | 3.3  | 3.4  | 4.1  | 4.2  | 4.3  | 1     | 2     | 3     | 4     | 5     |
    | ------- | ---- | ---- | :--: | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ----- | ----- | ----- | ----- | ----- |
    | 第一次  | X    | X    |  X   | X    | X    | X    | X    | X    | X    | X    | X    | X    | X    | 27.65 | 26.37 | 34.13 | 27.50 | 26.20 |
    | 第二次  |      |      |      |      |      |      |      |      |      |      |      |      |      | 27.80 | 26.58 |       |       |       |
    | 第三次  |      |      |      |      |      |      |      |      |      |      |      |      |      |       |       |       |       |       |
    |         |      |      |      |      |      |      |      |      |      |      |      |      |      |       |       |       |       |       |

# 2022/07/29-2022/07/31

+ 找出sf=10 original跑不了的原因。错误溯源定位：
  + 通过在POpActor.cpp中输出，检测到FileScan的operator无法工作。进一步发现仅有start信息
  + 通过在FileScan.cpp的start方法中输出，检测到ctx()在通知tuple的message时无法进行
  + 通过在POpContext.cpp的tell方法中输出，发现operatorActor\_->anon\_send无法正常工作，该类是POpActor。POpActor由Execution的actorSystem\_.spawn生成，而actorSystem\_在TestUtil.cpp的makeExecutor方法中生成，可能是CAF出问题了？？
  + 发现把FileScan、Shuffle、Filter都打开remoteSpawn后，可以正常工作。而单纯大概FileScan、Filter，会卡在Shuffle那里，推测是发送的压力太大或者单一节点的压力过大，导致CAF出故障。除了添加类型外，有可能能通过添加节点个数来解决该问题

# 2022/07/31-2022/08/02

+ nload/iftop 192.168.1.4\~6，看流量波动情况(有可能operator挂掉了？)
  + 用generated 1测试，发现FileScan -> Filter时，192.168.1.5/6 -> 192.168.1.4有较大的流量波动(接近100MB/s)
  + 用original 1.1测试，发现没有该现象
  + 说明很可能不是operator挂掉，而是根本就没有发出去
+ 看Execution.cpp中的boot.op->spawnOnRemote()有哪些operator是成立的
+ 新写一个message类，确定FileScan到Filter畅通
  + 新建了一个DebugMessage类
  + 在POpActor.cpp中新增了DEBUG类型，扔到on\_receive方法
  + 在POpContext中新增notifyDebug()方法，用于通知DebugMessage
  + 在FileScan.readAndSendTuples添加notifyDebug()
  + 在FilterPOp.on\_receive中新增DEBUG类型，用于证明存活
  + 在MessageSerialize.h中仿照其他message，新增DEBUG类型
  + 无法成功发送
+ 逐步确定Tuple的message情况(大小从小到大)
+ 把anon\_send换成同步的试试看

# 2022/08/02-2022/08/04

+ 核定在client-server在有多少个op的时候能正常工作，多少个不能(https://dl.acm.org/doi/abs/10.1145/2687357.2687363)
  + 在POpType.h中新增加了两个Type：SEND，RECEIVE
  + 在POpSerializer.h中仿照其他operator，添加SEND与RECEIVE
  + 在fpdb-executor/include/fpdb/executor/physical/下新增了test目录，里面是ReceivePOp.h与SendPOp.h
  + 在fpdb-executor/src/physical/下新增了test目录，里面是ReceivePOp.cpp与SendPOp.cpp
  + 在PrePToPTransformer.h中新增transformDebug()方法，在PrePToPTransformer.cpp中的transform方法中进行调用。其中，send在远程，receive在本地
  + 在Execution.boot()的allowedOpTypes中新增POpType::SEND
  + 在fpdb-executor/CmakeLists.txt中，仿照其他POp添加ReceivePOp与SendPOp

# 2022/08/04-2022/08/08

+ 验证是否是数据量太大的问题
  + sf=1时，更多的partition(lineorder=400)
    + 可以跑，速度慢了很多。1.1：75，1.2：74，1.3：74
  + sf=10时，更改TupleSet数据量
    + 让read的时候读同一个文件，方便修改数目
    + 110k行：跑不动
    + 90k行：能跑，115s
  + sf=10时，减少partition
  + sf=10时，限制读入的partition数目
    + 利用`lo_orderdate between ... and ...`来限制
    + 1：22，2：43，3：63，4：87，4.25、5：卡在了Aggregate(猜测，因为FileScan过了)
  
+ 看CAF(ylf的版本为0.18.5)的anon\_send接口，到socket层次。溯源：
  + libcaf\_core/caf/mixin/sender.hpp：anon\_send，发生在本地
  
  + libcaf\_core/caf/detail/profiled\_send.hpp：profiled\_send，发生在本地
  
  + libcaf\_core/src/blocking\_actor.cpp：enqueue，发生在本地。其规定了mailbox\_policy为：
  
    ```c++
      struct mailbox_policy {
        using deficit_type = size_t;
    
        using mapped_type = mailbox_element;
    
        using unique_pointer = mailbox_element_ptr;
    
        using queue_type
          = intrusive::wdrr_fixed_multiplexed_queue<policy::categorized,
                                                    normal_queue, urgent_queue>;
    
        static constexpr size_t normal_queue_index = 0;
    
        static constexpr size_t urgent_queue_index = 1;
      };
    ```
  
    因而，mailbox\_type为：`mailbox_type = intrusive::fifo_inbox<mailbox_policy>`
  
    + libcaf\_core/caf/intrusive/fifo\_inbox.hpp：synchronized_push_back，发生在本地
    + libcaf\_core/caf/intrusive/lifo\_inbox.hpp：synchronized_push_front -> push\_front，发生在本地。其pointer为mailbox\_element\*
  
  + libcaf\_core/src/scheduled\_actor.cpp：enqueue
  
    + libcaf\_core/caf/intrusive/fifo\_inbox.hpp：push_back
    + libcaf\_core/caf/intrusive/lifo\_inbox.hpp：push\_front
  
+ 由于push的时候找不到socket，故在提取的时候溯源：

  + libcaf\_core/caf/intrusive/lifo\_inbox.hpp：take\_head()，会把整个串接的链表给扔出来
  + libcaf\_core/caf/intrusive/fifo\_inbox.hpp：fetch\_more()，会不断把take\_head()中拿出的东西拼接在queue\_尾部
    + 其queue\_的类型为上述的wdrr…

# 2022/08/08-2022/08/11

+ 利用htop查看跑的时候内存占用情况(看一下能跑不能跑的情况)
+ 关注一下有没有其他的send(官方文档)
+ 一对一测试多大size的message发不出去
  + 修改DebugMessage，让其能够携带`vector<string>`，从而进行发送。通过读取文件之后在vector中多次push\_back的形式，形成较大规模的message
  + 256个15MB -> 3840MB能跑
  + 300个15MB -> 4500MB不能跑
+ 已经在官方上提issue，https://github.com/actor-framework/actor-framework/issues/1350。基于**CAF**的example：
  + 通过修改原有的example/remoting/remote\_spawn.cpp来实现。主要是将向calculator传输的信息改成string，remote\_spawn生成多个actor再一一发送读自文件的string
  + 注意，如果要发送一些自定类型，如`vector<string>`，需要在`CAF_ADD_TYPE_ID`中添加
  + 
  + string：
    + 1000个15MB -> 15000MB能跑
    + 1500个15MB -> 22500MB不能跑

# 2022/08/11-2022/08/14

+ 在服务器上写两个socket发15GB看会不会炸
  + 一行行地发是没问题的，而由于发送大小限制为int，所以不可能发送15GB(也没有那么长的char数组)
+ 看actor-framework/libcaf_core/caf/defaults.hpp，找一下这些参数的含义
  + string\_view：就是string
  + size\_t：unsigned long long
  + timespan：以ns为单位，实际分别为10s与30s
  + app\_identifier：
  + network\_backend：
  + max\_consecutive\_reads规定了最大读取的次数
    + libcaf\_core/src/actor\_system\_config.cpp
    + libcaf\_io/src/io/network/datagram\_handler.cpp：会将其传入max\_consecutive\_reads\_，在libcaf\_io/caf/io/network/datafram\_handler.hpp的handle_event_impl被使用(简称为mcr)。该函数会read_datagram至多mcr次
    + libcaf\_io/src/io/network/stream.cpp：将其传入max\_consecutive\_reads\_，在licaf\_io/caf/io/network/stream.hpp的handle_event_impl被使用(简称为mcr)。同理对读取这一行为有限制

# 2022/08/14-2022/08/19

+ buffer大小问题
  + 所有的message在哪汇总，汇总完怎么发送
  + 已经把看了的代码笔记写在CAF文件夹下

# 2022/08/24-2022/08/30

+ 找出hybrid的全流程(看一下原生代码)
  + WLFU(？)
  + CacheManagerActor
  + fpdb-calcite(可能)
  
  看了一些原生代码，笔记写在“原生代码解读.md”，已push到github上。

# 2022/08/30-2022/09/03

+ 验证下哪些operator能够在remote生成并使用

  |  Operator Type  | On Remote |
  | :-------------: | :-------: |
  |    AGGREGATE    |   True    |
  |     COLLATE     |   False   |
  |    FILE_SCAN    |   True    |
  |     FILTER      |   True    |
  |      GROUP      |   True    |
  | HASH_JOIN_BUILD |   True    |
  | HASH_JOIN_PROBE |   True    |
  |     SHUFFLE     |   True    |
  |      SORT       |   True    |

  collate的报错如下：

  ![](./imgs/collate fail.jpg)

# 2020/09/03-2020/09/04

+ 添加了外置控制是否remote spawn的模块
  + resources/config/mode.conf：含有所有op的mode信息
  + fpdb-executor/include/fpdb/executor/ModeCtrl.h：\_mapping用于建立op type与mode的映射
  + fpdb-executor/src/ModeCtrl.cpp：若external为true，则从mode.conf中读取信息，否则会采用默认配置
  + fpdb-executor/src/Execution.cpp：在execute()spawn前利用ModeCtrl来进行判断
  + fpdb-executor/CMakeLists.txt：添加ModeCtrl

# 2022/09/06-2022/09/011

+ CAF的CMake已修改，见https://github.com/thunderZH963/CAF-on-SmartNIC

+ 无sleep 30s，可能会出现空的handle，看一下是怎么回事

+ 1.先去更新CAF环境，再去跑新的数据集(10GB，确保有一个能跑就行)，用我的FPDB

+ huazhang(huazhang0518)；hefei：client；tianjin：server；smartNIC需要从tianjin连接

+ 2.从SQL树上叶子节点的op递增去测，在smartNIC与原有的都要测试，

+ 3.确认若FileScan + Shuffle，是否会出现两次数据传输(即从remote->local->remote)

+ 如果出现vscode remote无法连接，且有类似这样的报错：

  ```
  [19:16:13.494] > Acquiring lock on /home/huazhang/.vscode-server/bin/784b0177c56c607789f9638da7b6
  > bf3230d47a8c/vscode-remote-lock.huazhang.784b0177c56c607789f9638da7b6bf3230d47a8
  > c
  ```

  直接把该目录删了即可。原因是该目录被锁了
  
+ 在server端出现`./fpdb-main-server: error while loading shared libraries: libaws-cpp-sdk-core.so: cannot open shared object file: No such file or directory`。原因：没有把FPDB-build的lib加入到库路径中，加入即可(https://blog.csdn.net/mahoon411/article/details/113576586, https://blog.csdn.net/zhaohuaxicaishi/article/details/109617021)

  同时需要以下命令：

  ```
  sudo ln -sf /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/libcgraph.so.6.0.0 /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/libcgraph.so.6
  sudo ln -sf /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/libxdot.so.4.0.0 /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/libxdot.so.4
  sudo ln -sf /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/libcdt.so.5.0.0 /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/libcdt.so.5
  sudo ln -sf /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/libgvc.so.6.0.0 /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/libgvc.so.6
  sudo ln -sf /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/libpathplan.so.4.0.0 /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/libpathplan.so.4
  sudo ln -sf /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/graphviz/libgvplugin_core.so.6.0.0 /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/graphviz/libgvplugin_core.so.6
  sudo ln -sf /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/graphviz/libgvplugin_dot_layout.so.6.0.0 /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/graphviz/libgvplugin_dot_layout.so.6
  sudo ln -sf /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/graphviz/libgvplugin_neato_layout.so.6.0.0 /home/huazhang/Work/FPDB-bak-build/libs/graphviz_ep/install/lib/graphviz/libgvplugin_neato_layout.so.6
  ```

+ FileScan -> Filter -> Shuffle\_Filter -> HashJoinBuild/HashJoinProbe，HashJoinBuild -> HashJoinProbe -> Aggregate -> Aggregate\_Reduce -> collate，HashJoinProbe -> Shuffle\_Probe -> HashJoinProbe -> … -> Shuffle\_Probe -> Group -> Sort -> collate。实验后发现，aggregate在有group的时候不会出现，**原因不详**？？

  + 若没有group，则为build -> probe -> aggregate -> …
  + 若有group，则变成build -> probe -> shuffle -> group -> …，即shuffle似乎代替了aggregate的功能？

+ 跑不了可能是master node上8099端口之前残余的calcite，需要清掉

+ 数据软连接：

  ```
  sudo ln -s /root/nvme/Work/ssb-dbgen/data/ssb-sf1 /root/nvme/Work/FPDB-build/resources/metadata/ssb-sf1-sortlineorder/csv/data
  sudo ln -s /root/nvme/Work/ssb-dbgen/data/ssb-sf10 /root/nvme/Work/FPDB-build/resources/metadata/ssb-sf10-sortlineorder/csv/data
  ```

+ 应当予以去除的：

  + cmake里面caf是master而不是msater
  + FileScanPOp的debugMessage
  + POpContext的notifyDebug()
  + PrePToPTransformer的transformDebug(notifyDebug)

+ 利用SCP把当前东西复制给remote是很好用的

+ ```
  export PATH=$PATH:/root/nvme/java-11-openjdk-arm64/bin
  export JAVA_HOME=/root/nvme/java-11-openjdk-arm64/
  export J2SDKDIR=/root/nvme/java-11-openjdk-arm64/
  export MAVEN_HOME=/root/nvme/maven/
  export CLASSPATH=$CLASSPATH:$M2_HOME/lib
  export PATH=$PATH:$M2_HOME/bin
  export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib
  ```

+ 1GB，heifei-tianjin：

|                | 1.1    | 1.2    | 1.3    | 2.1    | 2.2    | 2.3    | 3.1    | 3.2    | 3.3    | 3.4    | 4.1    | 4.2    | 4.3    | 1     | 2     | 3     | 4     | 5     |
| -------------- | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ----- | ----- | ----- | ----- | ----- |
| FileScan       | 17.436 | 15.806 | 17.268 | 24.329 | 25.835 | 24.331 | 27.389 | 23.632 | 29.712 | 26.614 | 32.978 | 35.290 | 30.641 | 8.200 | 6.163 | 7.598 | 6.616 | 6.327 |
| +Filter        | 5.499  | 3.602  | 3.131  | 24.380 | 23.894 | 25.110 | 29.463 | 27.526 | 25.856 | 23.038 | 42.973 | 33.723 | 34.035 | 1.318 | 1.122 | 2.774 | 1.286 | 1.113 |
| +Shuffle       | 4.935  | 3.142  | 3.379  | 42.737 | 35.335 | 28.479 | 56.634 | 23.754 | 22.730 | 26.441 | 53.770 | 46.744 | 36.818 | 1.157 | 0.987 | 1.990 | 1.165 | 1.165 |
| +HashJoinBuild | 4.202  | 2.901  | 3.116  | 28.138 | 40.133 | 31.322 | 47.856 | 25.925 | 19.487 | 25.191 | 52.042 | 50.223 | 28.077 | 1.160 | 1.089 | 2.360 | 1.207 | 1.027 |
| +HashJoinProbe | 2.687  | 2.452  | 2.751  | 6.606  | 6.965  | 6.284  | 8.450  | 5.602  | 5.041  | 5.196  | 7.808  | 7.790  | 5.899  | 0.962 | 0.942 | 1.669 | 0.965 | 1.015 |
| +Aggregate     | 3.108  | 2.186  | 2.207  | 6.566  | 7.122  | 6.641  | 8.685  | 5.569  | 5.271  | 5.036  | 8.003  | 7.757  | 6.782  | 1.104 | 1.038 | 1.606 | 1.240 | 1.037 |
| +Group         | 2.491  | 2.146  | 2.218  | 6.421  | 6.941  | 6.455  | 7.687  | 5.550  | 5.128  | 5.046  | 7.281  | 7.415  | 5.836  | 1.000 | 0.855 | 1.094 | 0.914 | 0.968 |
| +Sort          | 2.474  | 2.196  | 2.197  | 6.280  | 6.844  | 6.353  | 7.266  | 5.694  | 5.086  | 4.957  | 7.080  | 7.124  | 5.961  | 0.995 | 0.933 | 1.255 | 0.887 | 0.952 |

+ 10GB，heifei-tianjin：
  + +Shuffle经常跑到一半就停了，原因不详

|                | 1.1     | 1.2     | 1.3     | 2.1     | 2.2     | 2.3     | 3.1     | 3.2     | 3.3     | 3.4     | 4.1     | 4.2     | 4.3     | 1      | 2      | 3      | 4      | 5      |
| -------------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------ | ------ | ------ | ------ | ------ |
| FileScan       | 255.310 | 238.096 | 257.219 | 415.158 | 426.168 | 446.984 | 528.520 | 461.330 | 494.274 | 305.058 | 454.773 | 388.108 | 337.865 | 48.661 | 45.026 | 55.778 | 42.862 | 56.360 |
| +Filter        | 249.005 | 237.320 | 240.067 | 423.584 | 556.668 | 456.024 | 509.329 | 534.980 | 427.526 | 379.466 | 505.562 | 437.664 | 427.281 | 38.555 | 37.210 | 43.022 | 39.808 | 38.399 |
| +Shuffle       | 249.795 | 234.154 | 243.307 | 802.356 | 760.374 | 727.243 | X       | X       |         |         | X       |         |         | 38.701 | 38.403 | 43.923 | 39.137 | 39.837 |
| +HashJoinBuild |         |         |         |         |         |         |         |         |         |         |         |         |         |        |        |        |        |        |
| +HashJoinProbe |         |         |         |         |         |         |         |         |         |         |         |         |         |        |        |        |        |        |
| +Aggregate     |         |         |         |         |         |         |         |         |         |         |         |         |         |        |        |        |        |        |
| +Group         |         |         |         |         |         |         |         |         |         |         |         |         |         |        |        |        |        |        |
| +Sort          |         |         |         |         |         |         |         |         |         |         |         |         |         |        |        |        |        |        |

+ 1GB，heifei-nic：

|                | 1.1    | 1.2    | 1.3    | 2.1    | 2.2    | 2.3    | 3.1    | 3.2    | 3.3    | 3.4    | 4.1    | 4.2    | 4.3    | 1     | 2     | 3     | 4     | 5     |
| -------------- | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ------ | ----- | ----- | ----- | ----- | ----- |
| FileScan       | 23.866 | 19.879 | 19.368 | 36.547 | 30.616 | 26.443 | 39.788 | 26.477 | 31.669 | 28.382 | 42.948 | 66.816 | 51.311 | 6.891 | 6.853 | 9.927 | 9.966 | 9.777 |
| +Filter        | 5.444  | 4.698  | 5.405  | 39.236 | 30.178 | 26.587 | 40.505 | 31.521 | 34.629 | 23.429 | 44.129 | 56.490 | 45.023 | 1.981 | 1.525 | 2.804 | 1.826 | 1.416 |
| +Shuffle       | 5.786  | 4.624  | 4.384  | 34.291 | 40.605 | 33.102 | 47.887 | 26.704 | 26.710 | 31.740 | 49.761 | 58.641 | 29.090 | 1.402 | 1.070 | 2.421 | 1.207 | 1.330 |
| +HashJoinBuild | 4.621  | 4.415  | 4.304  | 31.370 | 38.699 | 38.226 | 49.621 | 27.116 | 24.414 | 27.589 | 69.877 | 50.232 | 27.484 | 1.314 | 1.132 | 2.744 | 1304  | 1.144 |
| +HashJoinProbe | 4.608  | 4.729  | 4.845  | 14.913 | 15.462 | 14.277 | 17.374 | 12.961 | 13.116 | 13.639 | 20.270 | 17.986 | 15.283 | 1.109 | 1.018 | 1.767 | 1.108 | 1.064 |
| +Aggregate     | 4.563  | 4.186  | 4.199  | 15.235 | 14.560 | 14.586 | 18.075 | 13.238 | 12.888 | 13.093 | 18.093 | 22.627 | 15.294 | 1.103 | 0.984 | 1.667 | 1.075 | 1.088 |
| +Group         | 4.530  | 4.240  | 4.151  | 14.802 | 14.836 | 14.319 | 16.967 | 12.924 | 12.879 | 12.749 | 18.223 | 17.566 | 15.055 | 1.101 | 1.037 | 1.49  | 1.070 | 1.170 |
| +Sort          | 4.883  | 4.296  | 4.364  | 14.328 | 15.453 | 15.506 | 16.846 | 13.311 | 13.185 | 13.105 | 17.363 | 16.956 | 13.540 | 1.054 | 0.985 | 1.435 | 1.057 | 0.996 |

+ 10GB，heifei-nic：

|                | 1.1     | 1.2     | 1.3     | 2.1     | 2.2     | 2.3     | 3.1     | 3.2     | 3.3     | 3.4     | 4.1     | 4.2     | 4.3     | 1      | 2      | 3      | 4      | 5      |
| -------------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------ | ------ | ------ | ------ | ------ |
| FileScan       | 281.727 | 279.405 | 265.765 | 481.986 | 489.088 | 474.159 | 589.804 | 581.135 | 558.921 | 382.719 | 614.643 | 455.971 | 437.252 | 51.785 | 68.921 | 69.681 | 67.972 | 57.020 |
| +Filter        | 295.536 | 273.308 | 270.287 | 490.090 | 448.640 | 458.582 | 609.062 | 522.756 | 521.490 | 323.733 | 556.714 | 480.570 | 443.995 | 44.181 | 45.178 | 44.689 | 43.865 | 43.941 |
| +Shuffle       | 304.222 | 286.365 | 283.968 | X       |         |         | X       |         |         |         | X       |         |         | 45.469 | 45.701 | 48.966 | 44.054 | 46.628 |
| +HashJoinBuild | 300.614 | 283.125 | 288.429 | X       |         |         | X       |         |         |         | X       |         |         | 48.371 | 46.878 | 52.900 | 46.366 | 46.526 |
| +HashJoinProbe | 279.610 | 276.677 | 281.653 |         |         |         |         |         |         |         |         |         |         | 45.658 | 42.776 | 46.027 | 42.341 | 43.063 |
| +Aggregate     | 285.249 | 273.596 | 273.093 |         |         |         |         |         |         |         |         |         |         | 44.361 | 46.615 | 49.229 | 42.717 | 44.364 |
| +Group         | 296.698 | 270.948 | 276.316 |         |         |         |         |         |         |         |         |         |         | 42.353 | 42.429 | 48.755 | 41.469 | 42.181 |
| +Sort          | 280.520 | 272.066 | 275.601 | X       |         |         |         |         |         |         |         |         |         | 45.374 | 43.874 | 44.806 | 45.977 | 42.750 |

# 问题

+ 有一些op只能在第0台server上跑，如何解决？

# 未来工作

+ message机制要改掉，改成RDMA/dpdk -> NEON -> cache/pushdown管理

