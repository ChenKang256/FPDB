# 工作日志

# 2022/07/19

+ 更新filescanpop.cpp中的东西，自测一下，告知师兄！
  + 测一下`‘|’ and ','`：确实不行，会报错。原先的代码没有该问题，因为有这句话：`csvTableFormat->getFieldDelimiter()`
  + 自测不了：缺少`name(), table_->getSchema(), outputSchema`
+ remove all `ifdef __AVX2__`，有报错(S3)可以改CMake或代码，same
  + FileScanPOp.cpp：移除了ifdef
  + FileScanPOp.h：移除了ifdef
  + S3SelectPOp.h：移除了ifdef
  + S3SelectPOp.cpp：移除了ifdef与部分代码
  + S3GetPOp.h：移除了ifdef
  + S3GetPOp.cpp：移除了ifdef与部分代码
  + CSVToArrowSIMDChunkParser.h/.cpp：不需要管，涉及前六个文件的内容已删去
  + CSVToArrowSIMDStreamParser.h/.cpp：同上
  + SIMDParserHelper.h/.cpp：涉及到上面两组CSV文件，不需要管
  + 修改了fpdb-tuple下CMakeLists.txt的第44(CSVToArrowSIMDStreamParser)、46行(CSVToArrowSIMDChunkParser)、45行(SIMDParserHelper)

# 2022/07/20-2022/07/21

+ 统计CSV的算子数目、比例等情况。注意到，当前是pushdown-only的状态，所有的计算只在存储节点上运行。
  + 3与7比较类似，时间与比例都比较接近
  + 7与8形式相似，但是8显著慢于7，注意到8多了一个sum运算，猜测是sum运算开销太大导致速度下降
  + 7、8与9形式相似，但是9显著慢于前二者，这是因为前二者对时间有很强的限制，大大缩小了lineitem的范围，而lineitem作为最大的一个table，对性能影响比较显著，而9一方面hashjoin的范围很大，另一方面sum的求值范围很大，导致速度非常慢
  + 去除1的聚合，从33降低到17；去除8的所有sum，能将运行时间从34降到22；去除9的sum，能将运行时间从45降到26；去除12的sum，能将运行时间从9.8降到9.7。这能够说明sum对运行速度存在影响，并且sum需要的规模越大，对运行时间的影响也越大(百分比)。
  + 结论：普通的运算只要不涉及聚合运算与过于复杂的select、where、order by、group by等，速度都比较快；而涉及到sum、avg这些聚合运算，规模越大对运行时间的影响也越大(百分比)。
  + 感觉sort应该也有很大的影响，但是大家几乎都有sort的前提下，看不出来影响有多大。。
  + 难以看出提升的百分比与算子之间的关系
+ POpActor.cpp，看懂所有operator，client-server的交互(TPCHDIstTest.cpp的debug与test有助于了解)
  + 已在代码解读中完成

# 2022/07/22-2022/07/29

+ SPGLOG写

+ 了解plan的树状结构(输出每个sql的tree)

+ **捋清楚operator是如何在node上分布的，为何能够有性能提高(代码层次)**

  + 

+ scale 1,10,20 tpch size，1/2 node是什么情况，测试每一个sql(是否能够正常完成，为啥无法完成，哪些operator导致无法完成(client？))，纯无remote(注释掉filter与file\_scan)、仅有file\_scan。可以试着改变MainTest的doctest来进行。==优先完成==，用ssb测试
  + ssb生成坑点：
    + O\_CRATE的MODE问题：现在open如果使用了O\_CREAT，必须在后面给权限0777，否则会报错(bm\_utils.c的tbl\_open，retcode)
    + buffer overflow：修改shared.h的第120行，MAXAGG_LEN的定义，改成20。因为给的太小，所以造成overflow
  
+ scale生成：
  
  + stats.json：date不变(2557)，supplier乘上倍数，customer乘上倍数，part的200000加上600000\*log10()，lineorder接近乘上倍数
    + zoneMap.json：让lineorder按照划分数去划分lo\_orderdate，[19920101, 19980802]，2406天
    + schema.json：(这里都是指代)date = 1，supplier = 1，一个customer大概150000，一个part大概150000，一个lineorder大概150000
    + 原有的readme\_new是错误的，每次改变scale factor都必须修改，而不能简单地调用
  
  + 测试时间

    scale = 1，node = 2，有FileScan
  
    | sql名称 | 1.1  | 1.2  | 1.3  | 2.1   | 2.2   | 2.3   | 3.1   | 3.2   | 3.3  | 3.4  | 4.1   | 4.2   | 4.3   | 1    | 2    | 3    | 4    | 5    |
    | ------- | ---- | ---- | :--: | ----- | ----- | ----- | ----- | ----- | ---- | ---- | ----- | ----- | ----- | ---- | ---- | ---- | ---- | ---- |
    | 第一次  | 8.99 | 8.99 | 8.96 | 10.07 | 10.47 | 10.27 | 10.95 | 9.98  | 9.65 | 9.78 | 14.56 | 13.92 | 13.77 | 2.96 | 2.31 | 3.32 | 3.01 | 2.86 |
    | 第二次  | 9.20 | 8.90 | 9.34 | 10.37 | 10.75 | 10.39 | 10.26 | 9.74  | 9.67 | 9.28 | 14.67 | 14.19 | 13.77 | 3.01 | 2.29 | 3.23 | 2.92 | 2.81 |
    | 第三次  | 9.09 | 8.85 | 9.19 | 10.31 | 10.76 | 10.45 | 10.63 | 10.27 | 9.64 | 9.69 | 14.73 | 14.78 | 13.78 | 3.05 | 2.32 | 3.29 | 2.96 | 2.83 |
  | 平均    |      |      |      |       |       |       |       |       |      |      |       |       |       |      |      |      |      |      |
    
    scale = 10，node = 2，有FileScan
    
    | sql名称 | 1.1  | 1.2  | 1.3  | 2.1  | 2.2  | 2.3  | 3.1  | 3.2  | 3.3  | 3.4  | 4.1  | 4.2  | 4.3  | 1     | 2     | 3     | 4     | 5     |
    | ------- | ---- | ---- | :--: | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ----- | ----- | ----- | ----- | ----- |
    | 第一次  | X    | X    |  X   | X    | X    | X    | X    | X    | X    | X    | X    | X    | X    | 27.65 | 26.37 | 34.13 | 27.50 | 26.20 |
    | 第二次  |      |      |      |      |      |      |      |      |      |      |      |      |      | 27.80 | 26.58 |       |       |       |
    | 第三次  |      |      |      |      |      |      |      |      |      |      |      |      |      |       |       |       |       |       |
    |         |      |      |      |      |      |      |      |      |      |      |      |      |      |       |       |       |       |       |

# 2022/07/29-2022/07/31

+ 找出sf=10 original跑不了的原因。错误溯源定位：
  + 通过在POpActor.cpp中输出，检测到FileScan的operator无法工作。进一步发现仅有start信息
  + 通过在FileScan.cpp的start方法中输出，检测到ctx()在通知tuple的message时无法进行
  + 通过在POpContext.cpp的tell方法中输出，发现operatorActor\_->anon\_send无法正常工作，该类是POpActor。POpActor由Execution的actorSystem\_.spawn生成，而actorSystem\_在TestUtil.cpp的makeExecutor方法中生成，可能是CAF出问题了？？
  + 发现把FileScan、Shuffle、Filter都打开remoteSpawn后，可以正常工作。而单纯大概FileScan、Filter，会卡在Shuffle那里，推测是发送的压力太大或者单一节点的压力过大，导致CAF出故障。除了添加类型外，有可能能通过添加节点个数来解决该问题

# 2022/07/31-2022/08/02

+ nload/iftop 192.168.1.4\~6，看流量波动情况(有可能operator挂掉了？)
  + 用generated 1测试，发现FileScan -> Filter时，192.168.1.5/6 -> 192.168.1.4有较大的流量波动(接近100MB/s)
  + 用original 1.1测试，发现没有该现象
  + 说明很可能不是operator挂掉，而是根本就没有发出去
+ 看Execution.cpp中的boot.op->spawnOnRemote()有哪些operator是成立的
+ 新写一个message类，确定FileScan到Filter畅通
  + 新建了一个DebugMessage类
  + 在POpActor.cpp中新增了DEBUG类型，扔到on\_receive方法
  + 在POpContext中新增notifyDebug()方法，用于通知DebugMessage
  + 在FileScan.readAndSendTuples添加notifyDebug()
  + 在FilterPOp.on\_receive中新增DEBUG类型，用于证明存活
  + 在MessageSerialize.h中仿照其他message，新增DEBUG类型
  + 无法成功发送
+ 逐步确定Tuple的message情况(大小从小到大)
+ 把anon\_send换成同步的试试看

# 2022/08/02-2022/08/04

+ 核定在client-server在有多少个op的时候能正常工作，多少个不能(https://dl.acm.org/doi/abs/10.1145/2687357.2687363)
  + 在POpType.h中新增加了两个Type：SEND，RECEIVE
  + 在POpSerializer.h中仿照其他operator，添加SEND与RECEIVE
  + 在fpdb-executor/include/fpdb/executor/physical/下新增了test目录，里面是ReceivePOp.h与SendPOp.h
  + 在fpdb-executor/src/physical/下新增了test目录，里面是ReceivePOp.cpp与SendPOp.cpp
  + 在PrePToPTransformer.h中新增transformDebug()方法，在PrePToPTransformer.cpp中的transform方法中进行调用。其中，send在远程，receive在本地
  + 在Execution.boot()的allowedOpTypes中新增POpType::SEND
  + 在fpdb-executor/CmakeLists.txt中，仿照其他POp添加ReceivePOp与SendPOp

# 2022/08/04-2022/08/08

+ 验证是否是数据量太大的问题
  + sf=1时，更多的partition(lineorder=400)
    + 可以跑，速度慢了很多。1.1：75，1.2：74，1.3：74
  + sf=10时，更改TupleSet数据量
    + 让read的时候读同一个文件，方便修改数目
    + 110k行：跑不动
    + 90k行：能跑，115s
  + sf=10时，减少partition
  + sf=10时，限制读入的partition数目
    + 利用`lo_orderdate between ... and ...`来限制
    + 1：22，2：43，3：63，4：87，4.25、5：卡在了Aggregate(猜测，因为FileScan过了)
  
+ 看CAF(ylf的版本为0.18.5)的anon\_send接口，到socket层次。溯源：
  + libcaf\_core/caf/mixin/sender.hpp：anon\_send，发生在本地
  
  + libcaf\_core/caf/detail/profiled\_send.hpp：profiled\_send，发生在本地
  
  + libcaf\_core/src/blocking\_actor.cpp：enqueue，发生在本地。其规定了mailbox\_policy为：
  
    ```c++
      struct mailbox_policy {
        using deficit_type = size_t;
    
        using mapped_type = mailbox_element;
    
        using unique_pointer = mailbox_element_ptr;
    
        using queue_type
          = intrusive::wdrr_fixed_multiplexed_queue<policy::categorized,
                                                    normal_queue, urgent_queue>;
    
        static constexpr size_t normal_queue_index = 0;
    
        static constexpr size_t urgent_queue_index = 1;
      };
    ```
  
    因而，mailbox\_type为：`mailbox_type = intrusive::fifo_inbox<mailbox_policy>`
  
    + libcaf\_core/caf/intrusive/fifo\_inbox.hpp：synchronized_push_back，发生在本地
    + libcaf\_core/caf/intrusive/lifo\_inbox.hpp：synchronized_push_front -> push\_front，发生在本地。其pointer为mailbox\_element\*
  
  + libcaf\_core/src/scheduled\_actor.cpp：enqueue
  
    + libcaf\_core/caf/intrusive/fifo\_inbox.hpp：push_back
    + libcaf\_core/caf/intrusive/lifo\_inbox.hpp：push\_front
  
+ 由于push的时候找不到socket，故在提取的时候溯源：

  + libcaf\_core/caf/intrusive/lifo\_inbox.hpp：take\_head()，会把整个串接的链表给扔出来
  + libcaf\_core/caf/intrusive/fifo\_inbox.hpp：fetch\_more()，会不断把take\_head()中拿出的东西拼接在queue\_尾部
    + 其queue\_的类型为上述的wdrr…

# 2022/08/08-2022/08/11

+ 利用htop查看跑的时候内存占用情况(看一下能跑不能跑的情况)
+ 关注一下有没有其他的send(官方文档)
+ 一对一测试多大size的message发不出去
  + 修改DebugMessage，让其能够携带`vector<string>`，从而进行发送。通过读取文件之后在vector中多次push\_back的形式，形成较大规模的message
  + 256个15MB -> 3840MB能跑
  + 300个15MB -> 4500MB不能跑
+ 已经在官方上提issue，https://github.com/actor-framework/actor-framework/issues/1350。基于**CAF**的example：
  + 通过修改原有的example/remoting/remote\_spawn.cpp来实现。主要是将向calculator传输的信息改成string，remote\_spawn生成多个actor再一一发送读自文件的string
  + 注意，如果要发送一些自定类型，如`vector<string>`，需要在`CAF_ADD_TYPE_ID`中添加
  + 
  + string：
    + 1000个15MB -> 15000MB能跑
    + 1500个15MB -> 22500MB不能跑

# 2022/08/11

+ 在服务器上写两个socket发15GB看会不会炸

# 未来工作

+ message机制要改掉，改成RDMA/dpdk -> NEON -> cache/pushdown管理

